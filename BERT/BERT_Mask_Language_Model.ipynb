{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Masked Language Model (MLM)\n",
    "\n",
    "***is a key component of BERT, where certain words in a sentence are masked, and the model learns to predict them based on context. This technique enables the model to understand relationships between words and improve its language comprehension capabilities.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The BERT-base-uncased model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***The BERT-base-uncased model is a pre-trained transformer model developed by Google. It's widely used for natural language understanding tasks like text classification, sentiment analysis, and question answering. The \"uncased\" version means it does not differentiate between uppercase and lowercase letters. BERT-base has 12 layers, 768 hidden units, and 12 attention heads, making it powerful yet efficient for various NLP applications.***\n",
    "\n",
    "## BERT-base has 12 layers, 768 hidden units, and 12 attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BERT Model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31NTU2dQSukq"
   },
   "source": [
    "# BERT base model (uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-4a7LpFQDEM",
    "outputId": "e20ef9d3-1652-4d85-c546-12c13b5c0dc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91956\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Predicted words: ['home']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "#Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to predict masked words\n",
    "def predict_masked_words(sentence):\n",
    "    # Tokenize the input sentence\n",
    "    tokenized_input = tokenizer.encode_plus(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    # Get the position of the masked token\n",
    "    masked_index = torch.where(tokenized_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "\n",
    "    # Get the logits for the masked token\n",
    "    predictions = outputs.logits[0, masked_index, :]\n",
    "\n",
    "    # Get the top predictions\n",
    "    top_indices = torch.topk(predictions, 1, dim=1).indices[0].tolist()\n",
    "\n",
    "    # Convert token IDs to actual words\n",
    "    predicted_tokens = [tokenizer.decode([index]) for index in top_indices]\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "# Example sentence with a masked word\n",
    "input_sentence = \"I want to go [MASK].\"\n",
    "\n",
    "# Predict masked words\n",
    "predicted_words = predict_masked_words(input_sentence)\n",
    "\n",
    "print(\"Predicted words:\", predicted_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT large model (uncased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The BERT-large-uncased model has the following configuration: 24 layers, 1024 hidden dimensions, 16 attention heads, and 336 million parameters. This larger version of BERT provides enhanced performance for various NLP tasks. The \"uncased\" model does not differentiate between uppercase and lowercase letters. It is particularly suited for complex language understanding applications.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtIwxG8xS0jd"
   },
   "source": [
    "\n",
    "## 24 layers, 1024 hidden dimensions, 16 attention heads, and 336 million parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCIkOwOsQOcj",
    "outputId": "c324931b-3ea5-47a6-e503-fddcde085532"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted words: ['?', 'die', 'sleep', '...', 'stay']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to predict masked words\n",
    "def predict_masked_words(sentence):\n",
    "    # Tokenize the input sentence\n",
    "    tokenized_input = tokenizer.encode_plus(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    # Get the position of the masked token\n",
    "    masked_index = torch.where(tokenized_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "\n",
    "    # Get the logits for the masked token\n",
    "    predictions = outputs.logits[0, masked_index, :]\n",
    "\n",
    "    # Get the top 5 predictions\n",
    "    top_5_indices = torch.topk(predictions, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "    # Convert token IDs to actual words\n",
    "    predicted_tokens = [tokenizer.decode([index]) for index in top_5_indices]\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "# Example sentence with a masked word\n",
    "input_sentence = \"Are you going to [MASK].\"\n",
    "# Predict masked words\n",
    "predicted_words = predict_masked_words(input_sentence)\n",
    "\n",
    "print(\"Predicted words:\", predicted_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnMS7paJRdaN"
   },
   "source": [
    "# BERT multilingual base model (cased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcMvSe4TTC0k"
   },
   "source": [
    "***The BERT multilingual base model (cased) supports 104 languages, making it versatile for global NLP applications. It has 12 layers, 768 hidden dimensions, 12 attention heads, and 110 million parameters. The \"cased\" version maintains case sensitivity, distinguishing between uppercase and lowercase letters. This model is ideal for tasks requiring nuanced understanding across multiple languages.***\n",
    "\n",
    "##  It has 12 layers, 768 hidden dimensions, 12 attention heads, and 110 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U_6RQbGFTCG4",
    "outputId": "d4b0701c-318e-4d48-a9b6-8374fefcf3a2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5658b7b44e2943b786003cdafa9ff0ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91956\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\91956\\.cache\\huggingface\\hub\\models--bert-base-multilingual-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6aaf3307cf4879a7fac5b857f7585a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafd0e268b8d4eb5b3bc67d93e033df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91956\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2902c1ce1a46509651421fff46e7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ee7be140d7476e9c321a731a96280b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted words: ['game', 'food', 'thing', 'song', 'movie']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-multilingual-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to predict masked words\n",
    "def predict_masked_words(sentence):\n",
    "    # Tokenize the input sentence\n",
    "    tokenized_input = tokenizer.encode_plus(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "    # Get the position of the masked token\n",
    "    masked_index = torch.where(tokenized_input[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Predict the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_input)\n",
    "\n",
    "    # Get the logits for the masked token\n",
    "    predictions = outputs.logits[0, masked_index, :]\n",
    "\n",
    "    # Get the top 5 predictions\n",
    "    top_5_indices = torch.topk(predictions, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "    # Convert token IDs to actual words\n",
    "    predicted_tokens = [tokenizer.decode([index]) for index in top_5_indices]\n",
    "\n",
    "    return predicted_tokens\n",
    "\n",
    "# Example sentence with a masked word\n",
    "input_sentence = \"What is your favorite [MASK]?\"\n",
    "# Predict masked words\n",
    "predicted_words = predict_masked_words(input_sentence)\n",
    "\n",
    "print(\"Predicted words:\", predicted_words)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
